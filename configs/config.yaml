# This is the single, unified configuration file for your htr-vit-iam project.
# You can modify all training and model parameters here.

# seed for random number generators in pytorch, numpy and python.random
seed: 42

# Task name determines the output directory path.
# e.g., logs/train_htr_iam/
task_name: "train_htr_rimes"

# Set to False to skip training and only run testing.
# Set `ckpt_path` to the model you want to test.
train: True
test: True

# Path to a checkpoint file to resume training or for testing.
# e.g., "logs/train_htr_iam/runs/2023-10-27_10-00-00/checkpoints/last.ckpt"
ckpt_path: null


# Paths Configuration
# Defines all the necessary directories for the project.
paths:
  root_dir: "."
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  output_dir: ${paths.log_dir}/${task_name}/runs/${now:%Y-%m-%d_%H-%M-%S}
  work_dir: ${hydra:runtime.cwd}

# PyTorch Lightning Trainer Configuration
# See https://lightning.ai/docs/pytorch/stable/common/trainer.html
trainer:
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${paths.output_dir}
  min_epochs: 1
  max_epochs: 150
  accelerator: gpu 
  devices: [2]
  deterministic: False # Should be False for CTC as per original repo notes
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 0

# Data Module Configuration
# Configures the data loading for train, validation, and test sets.
# Data Module Configuration
data:
  _target_: src.data.htr_datamodule.HTRDataModule
  tokenizer: ${tokenizer}

  train_config:
    _target_: src.data.data_config.DataConfig
    datasets:
      rimes:
        _target_: src.data.data_config.DatasetConfig
        name: rimes
        images_path: ${paths.data_dir}/htr_datasets/rimes/RIMES-2011-Lines/Images/
        labels_path: ${paths.data_dir}/htr_datasets/rimes/RIMES-2011-Lines/Transcriptions/
        splits_path: ${paths.data_dir}/htr_datasets/rimes/RIMES-2011-Lines/Sets/train.txt
        read_data: src.data.data_utils.read_data_rimes
    stage: train
    img_size: [64, 512]
    binarize: True
    batch_size: 64
    num_workers: 8
    pin_memory: True
    vocab_path: ${paths.data_dir}/vocab.txt
    transforms:
      - _target_: torchvision.transforms.ToTensor
        _args_: []

  val_config:
    _target_: src.data.data_config.DataConfig
    datasets:
      rimes:
        _target_: src.data.data_config.DatasetConfig
        name: rimes
        images_path: ${paths.data_dir}/htr_datasets/rimes/RIMES-2011-Lines/Images/
        labels_path: ${paths.data_dir}/htr_datasets/rimes/RIMES-2011-Lines/Transcriptions/
        splits_path: ${paths.data_dir}/htr_datasets/rimes/RIMES-2011-Lines/Sets/val.txt
        read_data: src.data.data_utils.read_data_rimes
    stage: val
    img_size: ${data.train_config.img_size}
    binarize: ${data.train_config.binarize}
    batch_size: ${data.train_config.batch_size}
    num_workers: ${data.train_config.num_workers}
    pin_memory: True
    vocab_path: ${paths.data_dir}/vocab.txt
    transforms:
      - _target_: torchvision.transforms.ToTensor
        _args_: []

  test_config:
    _target_: src.data.data_config.DataConfig
    datasets:
      rimes:
        _target_: src.data.data_config.DatasetConfig
        name: rimes
        images_path: ${paths.data_dir}/htr_datasets/rimes/RIMES-2011-Lines/Images/
        labels_path: ${paths.data_dir}/htr_datasets/rimes/RIMES-2011-Lines/Transcriptions/
        splits_path: ${paths.data_dir}/htr_datasets/rimes/RIMES-2011-Lines/Sets/test.txt
        read_data: src.data.data_utils.read_data_rimes
    stage: test
    img_size: ${data.train_config.img_size}
    binarize: ${data.train_config.binarize}
    batch_size: ${data.train_config.batch_size}
    num_workers: ${data.train_config.num_workers}
    pin_memory: True
    vocab_path: ${paths.data_dir}/vocab.txt
    transforms:
      - _target_: torchvision.transforms.ToTensor
        _args_: []


# Model Configuration
# This points to our htr_vit model.
model:
  _target_: src.models.crnn_ctc_module.CRNN_CTC_Module
  _logger : ${logger}

  # Pass the tokenizer config to the model
  tokenizer: ${tokenizer}

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-5
    weight_decay: 0.5
  scheduler: null
  net:
    _target_: src.models.components.htr_vit.MaskedAutoencoderViT
    img_size: ${data.train_config.img_size}
    patch_size: [64, 4]
    embed_dim: 768
    depth: 4
    num_heads: 6
    mlp_ratio: 4
    tokenizer : ${tokenizer}
  compile: false
  # These are passed to the Lightning Module but not defined in the original model yaml
  datamodule : ${data}
  log_val_metrics: false

# Tokenizer Configuration
tokenizer:
  _target_: src.data.components.tokenizers.CharTokenizer
  model_name: "char_tokenizer"
  vocab_file: ${paths.data_dir}/vocab.txt

# Callbacks Configuration
# Defines actions to be taken during training, like saving checkpoints.
callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch_{epoch:03d}"
    monitor: "val/mean_cer" # Monitor mean Character Error Rate across all val datasets
    mode: "min"
    save_last: True
    save_top_k: 1
    auto_insert_metric_name: False
  
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: "val/mean_cer" # Monitor mean Character Error Rate
    min_delta: 0.001
    patience: 20 # Stop if no improvement after 20 validation checks
    verbose: True
    mode: "min"

# Logger Configuration
# We disable external loggers like wandb for simplicity.
# Logger Configuration
logger:
  wandb:
    _target_: lightning.pytorch.loggers.WandbLogger
    name: ${task_name} # Sets the run name, e.g., "train_htr_rimes"
    save_dir: ${paths.output_dir}
    project: "HTR-ViT"  # <--- SET YOUR W&B PROJECT NAME HERE
    # Limit artefact size: don't upload checkpoints to W&B (they remain on local disk).
    log_model: false
    prefix: ""

extras:
  ignore_warnings: True
  enforce_tags: False
  print_config: True

# Hydra settings - needed for the @hydra.main decorator to work
hydra:
  run:
    dir: ${paths.output_dir}
  sweep:
    dir: ${paths.output_dir}
    subdir: ${hydra.job.num}
  job:
    name: ${task_name}
    chdir: false 