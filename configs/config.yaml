# This is the single, unified configuration file for your htr-vit-iam project.
# You can modify all training and model parameters here.

# seed for random number generators in pytorch, numpy and python.random
seed: 42

# Task name determines the output directory path.
# e.g., logs/train_htr_iam/
task_name: "train_htr_sharada"

# Set to False to skip training and only run testing.
# Set `ckpt_path` to the model you want to test.
train: True
test: True

# Path to a checkpoint file to resume training or for testing.
# e.g., "logs/train_htr_iam/runs/2023-10-27_10-00-00/checkpoints/last.ckpt"
ckpt_path: null


# Paths Configuration
# Defines all the necessary directories for the project.
paths:
  root_dir: ${hydra:runtime.cwd}
  data_dir: /ssd_scratch/jyothi.swaroopa/Simran/data
  log_dir: ${paths.root_dir}/logs/
  output_dir: ${paths.log_dir}/${task_name}/runs/${now:%Y-%m-%d_%H-%M-%S}
  work_dir: ${hydra:runtime.cwd}

# PyTorch Lightning Trainer Configuration
# See https://lightning.ai/docs/pytorch/stable/common/trainer.html
trainer:
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${paths.output_dir}
  min_epochs: 1
  max_epochs: 4
  accelerator: gpu 
  devices: [1]
  precision: "16-mixed" # Use mixed precision for speed
  deterministic: False
  gradient_clip_val: 1.0
  check_val_every_n_epoch: null # Disable epoch-based validation
  val_check_interval: 500 
  num_sanity_val_steps: 0

# Data Module Configuration
# Configures the data loading for train, validation, and test sets.
data:
  _target_: src.data.htr_datamodule.HTRDataModule
  tokenizer: ${tokenizer}

  train_config:
    _target_: src.data.data_config.DataConfig
    datasets:
      sharada:
        _target_: src.data.data_config.DatasetConfig
        name: sharada
        images_path: ${paths.data_dir}
        labels_path: ${paths.data_dir} # Not used by sharada reader, but good practice
        splits_path: ${paths.data_dir}/train_sharada.txt
        read_data: src.data.data_utils.read_data_sharada
    stage: train
    img_size: [64, 768]
    binarize: False
    batch_size: 256 # Increase batch size for better GPU utilization
    num_workers: 16 # Increase workers to prevent data bottlenecks
    pin_memory: True
    vocab_path: ${paths.data_dir}/sharada_vocab.txt
    transforms:
      - _target_: torchvision.transforms.ToTensor
        _args_: []
      - _target_: torchvision.transforms.Normalize
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

  val_config:
    _target_: src.data.data_config.DataConfig
    datasets:
      sharada:
        _target_: src.data.data_config.DatasetConfig
        name: sharada
        images_path: ${paths.data_dir}
        labels_path: ${paths.data_dir}
        splits_path: ${paths.data_dir}/val_sharada.txt
        read_data: src.data.data_utils.read_data_sharada
    stage: val
    img_size: ${data.train_config.img_size}
    binarize: ${data.train_config.binarize}
    batch_size: ${data.train_config.batch_size}
    num_workers: ${data.train_config.num_workers}
    pin_memory: True
    vocab_path: ${paths.data_dir}/sharada_vocab.txt
    transforms:
      - _target_: torchvision.transforms.ToTensor
        _args_: []
      - _target_: torchvision.transforms.Normalize
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

  test_config:
    _target_: src.data.data_config.DataConfig
    datasets:
      sharada:
        _target_: src.data.data_config.DatasetConfig
        name: sharada
        images_path: ${paths.data_dir}
        labels_path: ${paths.data_dir}
        splits_path: ${paths.data_dir}/test_sharada.txt
        read_data: src.data.data_utils.read_data_sharada
    stage: test
    img_size: ${data.train_config.img_size}
    binarize: ${data.train_config.binarize}
    batch_size: ${data.train_config.batch_size}
    num_workers: ${data.train_config.num_workers}
    pin_memory: True
    vocab_path: ${paths.data_dir}/sharada_vocab.txt
    transforms:
      - _target_: torchvision.transforms.ToTensor
        _args_: []
      - _target_: torchvision.transforms.Normalize
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]


# Model Configuration
# This points to our htr_vit model.
model:
  _target_: src.models.crnn_ctc_module.CRNN_CTC_Module
  _logger : ${logger}

  # Pass the tokenizer config to the model
  tokenizer: ${tokenizer}

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-4
    weight_decay: 0.5
  scheduler: null
  net:
    _target_: src.models.components.htr_vit.MaskedAutoencoderViT
    img_size: ${data.train_config.img_size}
    patch_size: [64, 4]
    embed_dim: 768
    depth: 4
    num_heads: 6
    mlp_ratio: 4
    tokenizer : ${tokenizer}
  compile: True # Use torch.compile for a significant speedup
  # These are passed to the Lightning Module but not defined in the original model yaml
  datamodule : ${data}
  log_val_metrics: false

# Tokenizer Configuration
tokenizer:
  _target_: src.data.components.tokenizers.CharTokenizer
  model_name: "char_tokenizer"
  vocab_file: ${paths.data_dir}/sharada_vocab.txt

# Callbacks Configuration
# Defines actions to be taken during training, like saving checkpoints.
callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}/checkpoints
    filename: "step_{step}-cer_{val/mean_cer:.4f}"
    monitor: "val/mean_cer"
    mode: "min"
    every_n_train_steps: 5000 # Save a checkpoint every 5000 training steps
    save_top_k: 3 # Save the top 3 best checkpoints
    save_last: True # Also save the latest checkpoint
    auto_insert_metric_name: False
  
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar
  
  early_stopping: null

# Logger Configuration
# We disable external loggers like wandb for simplicity.
# Logger Configuration
logger:
  wandb:
    _target_: lightning.pytorch.loggers.WandbLogger
    name: ${task_name} # Sets the run name, e.g., "train_htr_rimes"
    save_dir: ${paths.output_dir}
    project: "HTR-ViT"  # <--- SET YOUR W&B PROJECT NAME HERE
    # Limit artefact size: don't upload checkpoints to W&B (they remain on local disk).
    log_model: false
    prefix: ""

extras:
  ignore_warnings: True
  enforce_tags: False
  print_config: True

# Hydra settings - needed for the @hydra.main decorator to work
hydra:
  run:
    dir: ${paths.output_dir}
  sweep:
    dir: ${paths.output_dir}
    subdir: ${hydra.job.num}
  job:
    name: ${task_name}
    chdir: false 